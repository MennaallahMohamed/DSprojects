{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# menna allah mohamed abd el samiea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# description:\n",
    "    I was supposed to make a model to classify whether the x-ray of this lung is normal or suffers from pneumonia.\n",
    "    But I pressed for the lack of time. I am very sorry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Visual C++ Redistributable is not installed, this may lead to the DLL load failure.\n",
      "                 It can be downloaded at https://aka.ms/vs/16/release/vc_redist.x64.exe\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Data Visualizations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(font_scale= 2)\n",
    "\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "#!pip install torch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, ConcatDataset , Dataset\n",
    "from torchvision import transforms, datasets, models\n",
    "from PIL import Image, ImageOps\n",
    "from torchvision.models import resnet18, resnet34\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU:{torch.cuda.get_device_name()}\")\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remainig_time(tt):\n",
    "    if tt/60 < 1:\n",
    "        return \"{} sec\".format(int(tt))\n",
    "    if tt/60/60 < 1:\n",
    "        return \"{} min\".format(int(tt/60))\n",
    "    if tt/60/60/24 < 1:\n",
    "        mins = tt/60\n",
    "        hrs = mins/60\n",
    "        return \"{:.2f} hrs\".format(hrs)\n",
    "    if tt/60/60/24 > 1:\n",
    "        days = tt/60/60/24\n",
    "        return \"{:.2f} days\".format(days)\n",
    "\n",
    "\n",
    "def save_model(model, optim, loss_train, loss_val, acc_train, acc_val, epoch, model_name, model_path=''):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optim.state_dict(),\n",
    "        'loss_train': loss_train,\n",
    "        'loss_val': loss_val,\n",
    "        'epoch': epoch,\n",
    "        'acc_train': acc_train,\n",
    "        'acc_val': acc_val,\n",
    "    }, os.path.join(model_path, model_name))\n",
    "\n",
    "\n",
    "def load_model(model_path, device=DEVICE):\n",
    "    return torch.load(model_path, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters (You can change as you like ... and see how it affects the results)\n",
    "LR = 3e-5\n",
    "BATCH_SIZE = 32\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224\n",
    "IMG_RESIZE = (IMG_HEIGHT, IMG_WIDTH)\n",
    "EPOCHS = 100\n",
    "mean=0.5\n",
    "std=0.5 \n",
    "\n",
    "# Constants\n",
    "NUM_WORKERS = 3 # Based on the \n",
    "NUM_CLASSES = 2\n",
    "CLASSES = ['NORMAL', 'PNEUMONIA']\n",
    "CHANNELS = 1\n",
    "MIN_ACC = float('-inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation\n",
    "\n",
    "TR = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size= IMG_RESIZE),\n",
    "    transforms.Normalize(.5, std)\n",
    "])\n",
    "    # 2- Resize the Image,#\n",
    "    # 3- Convert Image data into tensor,#\n",
    "    # 4- Normalize your Image with mean=0.5, std=0.5 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'C:\\\\Users\\\\LENOVO\\\\chest_xray\\\\chest_xray'\n",
    "TRAIN_DATA_PATH = os.path.join(DATA_PATH, 'train/')\n",
    "VAL_DATA_PATH = os.path.join(DATA_PATH, 'val/')\n",
    "TEST_DATA_PATH = os.path.join(DATA_PATH, 'test/')\n",
    "\n",
    "train_normal = glob.glob(TRAIN_DATA_PATH + \"NORMAL/*\")\n",
    "train_pneumonia = glob.glob(TRAIN_DATA_PATH + \"PNEUMONIA/*\")\n",
    "\n",
    "test_normal = glob.glob(TEST_DATA_PATH + \"NORMAL/*\")\n",
    "test_pneumonia = glob.glob(TEST_DATA_PATH + \"PNEUMONIA/*\")\n",
    "\n",
    "\n",
    "val_normal = glob.glob(VAL_DATA_PATH + \"NORMAL/*\")\n",
    "val_pneumonia = glob.glob(VAL_DATA_PATH + \"PNEUMONIA/*\")\n",
    "\n",
    "train = datasets.ImageFolder(root = TRAIN_DATA_PATH , transform = TR )\n",
    "val = datasets.ImageFolder(root = VAL_DATA_PATH , transform = TR )\n",
    "df_test = datasets.ImageFolder(root = TEST_DATA_PATH , transform = TR)\n",
    "\n",
    "df_train = ConcatDataset([train,val])\n",
    "\n",
    "\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(df_train.datasets[0].imgs + df_train.datasets[1].imgs ,columns =[\"path\",\"label\"]) \n",
    "df_test = pd.DataFrame(df_test.imgs , columns = [\"path\" , \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images in Training set: 5232\n",
      "Images in Testing set:  624\n"
     ]
    }
   ],
   "source": [
    "print(\"Images in Training set:\" , df_train.shape[0])\n",
    "print(\"Images in Testing set: \" ,df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>C:\\Users\\LENOVO\\chest_xray\\chest_xray\\train/NO...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2685</th>\n",
       "      <td>C:\\Users\\LENOVO\\chest_xray\\chest_xray\\train/PN...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>C:\\Users\\LENOVO\\chest_xray\\chest_xray\\train/NO...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   path  label\n",
       "585   C:\\Users\\LENOVO\\chest_xray\\chest_xray\\train/NO...      0\n",
       "2685  C:\\Users\\LENOVO\\chest_xray\\chest_xray\\train/PN...      1\n",
       "96    C:\\Users\\LENOVO\\chest_xray\\chest_xray\\train/NO...      0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x293e34321c0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot( x = 'label' , data = df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7411, 0.2589])\n"
     ]
    }
   ],
   "source": [
    "## Apply Weighted Loss w.r.t Size of label\n",
    "## So class with low number of images get higher weight\n",
    "## while class with High number of images get lower weight\n",
    "w = [1345, 3850]\n",
    "weight = torch.FloatTensor([1 - 1345/sum(w) , 1 - 3850/sum(w)])\n",
    "criterion = nn.NLLLoss(weight= weight).to(DEVICE)\n",
    "# Show the Weights (should be close to this ==> [0.74, 0.25])\n",
    "print(criterion.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-eb094b513877>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-19-eb094b513877>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    imgs, labels = # Get Your image and targets from the given batch\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def train_fn(batches, model, optim, scheduler, device=DEVICE):\n",
    "    model.train() # set the model mode => training\n",
    "    batch_acc = 0\n",
    "    ep_loss = 0\n",
    "    # Loop through the training batches\n",
    "    for batch in tqdm(batches, total=len(batches), position=0, leave=True):\n",
    "        \n",
    "        imgs, labels = # Get Your image and targets from the given batch\n",
    "        # Forward Propagation\n",
    "        labels_pred = ## Get Your predictions from model\n",
    "        # Calculate Loss\n",
    "        loss = ## Get your loss bet. Predictions and Targets\n",
    "        # Backward propagation (Check: https://discuss.pytorch.org/t/what-does-the-backward-function-do/9944)\n",
    "        ## - Zero your optimizer gradients\n",
    "        ## - Calculate loss gradient\n",
    "        ## - Make step with optimizer\n",
    "        ## - Accumulating Loss & Accuracy Across batches\n",
    "        ep_loss += loss.item()\n",
    "        batch_acc += sum(labels == labels_pred.argmax(1)).item()\n",
    "    # Calculate The whole Epoch Accuracy after the batches loop ends\n",
    "    ep_acc = batch_acc / (BATCH_SIZE * len(batches))\n",
    "    ## Return the ep_loss and the ep_acc\n",
    "    return ep_loss, ep_acc\n",
    "Evaluation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# thank you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
